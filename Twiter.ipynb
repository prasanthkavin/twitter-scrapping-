{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnYrjCuhb85rTtbOettV9g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prasanthkavin/twitter-scrapping-/blob/main/Twiter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AtoVBmGlxcH"
      },
      "outputs": [],
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import pymongo\n",
        "import datetime\n",
        "from datetime import date\n",
        "import time\n",
        "\n",
        "\n",
        "st.header(\"\"\"\n",
        "Welcome to twitter scraping\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "search = st.text_input('Enter the Search Term')\n",
        "count = st.slider('Slide the limit',10,1000)\n",
        "start_date=st.date_input('Pick start date',datetime.date(2022, 1, 1))\n",
        "\n",
        "end_date=st.date_input('Pick end date',datetime.date(2022,1,1))\n",
        "\n",
        "count = int(count)\n",
        "submit = st.button(label='Search')\n",
        "\n",
        "tweets_list1 = []\n",
        "\n",
        "if submit:\n",
        "    for i , tweet in enumerate(sntwitter.TwitterSearchScraper(f'{search} since:{start_date} until:{end_date}').get_items()):\n",
        "        if i > count-1:\n",
        "            break\n",
        "        tweets_list1.append(\n",
        "            [tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.likeCount, tweet.retweetCount,\n",
        "             tweet.sourceLabel, tweet.user.location])\n",
        "tweet_df = pd.DataFrame(tweets_list1, columns=[\"Date\", \"Id\", \"Content\", \"Username\", \"LikeCount\", \"RetweetCount\",\n",
        "                                                       \"SourceLabel\", \"Location\"])\n",
        "\n",
        "st.dataframe(tweet_df,)\n",
        "\n",
        "file_converted = tweet_df.to_csv()\n",
        "st.download_button(\n",
        "            label=\"Download data as CSV\",\n",
        "            data=file_converted,\n",
        "            file_name='Data.csv',\n",
        "            mime='text/csv',\n",
        "        )\n",
        "file_converted = tweet_df.to_json()\n",
        "st.download_button(\n",
        "            label=\"Download data as json\",\n",
        "            data=file_converted,\n",
        "            file_name='Data.json',\n",
        "            mime=\"application/json\",\n",
        "        )\n",
        "export = st.button(label=\"export\")\n",
        "now = datetime.datetime.now()\n",
        "client = pymongo.MongoClient(\"mongodb+srv://kavin:kavin@cluster0.ddhrzmu.mongodb.net/?retryWrites=true&w=majority\")\n",
        "db = client.Twitter\n",
        "records = db.data\n",
        "for i, tweet in enumerate(\n",
        "                sntwitter.TwitterSearchScraper(f'{search} since:{start_date} until:{end_date}').get_items()):\n",
        "    if i > count :\n",
        "        break\n",
        "    tweets_list1.append(\n",
        "                [tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.likeCount, tweet.retweetCount,\n",
        "                 tweet.sourceLabel, tweet.user.location])\n",
        "    tweet_df = pd.DataFrame(tweets_list1,\n",
        "                                    columns=[\"Date\", \"Id\", \"Content\", \"Username\", \"LikeCount\", \"RetweetCount\",\n",
        "                                             \"SourceLabel\", \"Location\"])\n",
        "l = {\"Scraped_Name\": search, \"Time\": now, \"Scraped_data\": [\n",
        "                    {\"Date_Time\": tweet.date, \"Tweet_ID\": tweet.id, \"Tweet_content\": tweet.content,\n",
        "                     \"Username\": tweet.user.username,\n",
        "                     \"Like Count\": tweet.likeCount, \"ReTweet Count\": tweet.retweetCount, \"Source\": tweet.sourceLabel,\n",
        "                     \"Location\": tweet.user.location}]}\n",
        "records.insert_one(l)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}